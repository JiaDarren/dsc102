{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 DSC 102 2022 FA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this assignment we will conduct data engineering for the Amazon dataset. It is divided into 2 parts. The extracted features in Part 1 will be used for the Part 2 of assignment, where you train a model (or models) to predict user ratings for a product.\n",
    "\n",
    "We will be using Apache Spark for this assignment. The default Spark API will be DataFrame, as it is now the recommended choice over the RDD API. That being said, please feel free to switch back to the RDD API if you see it as a better fit for the task. We provide you an option to request RDD format to start with. Also you can switch between DataFrame and RDD in your solution. \n",
    "\n",
    "Another newer API is Koalas, which is also avaliable. However, it has constraints and is not applicable to most tasks. Refer to the PA statement for detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the following parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T18:47:30.022030Z",
     "start_time": "2020-02-03T18:47:30.019499Z"
    }
   },
   "outputs": [],
   "source": [
    "PID = 'a16195691' # your pid, for instance: 'a43223333'\n",
    "INPUT_FORMAT = 'dataframe' # choose a format of your input data, valid options: 'dataframe', 'rdd', 'koalas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T18:50:16.780690Z",
     "start_time": "2020-02-03T18:47:30.263681Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Found pyspark version \"3.3.1\" installed. The pyspark version 3.2 and above has a built-in \"pandas APIs on Spark\" module ported from Koalas. Try `import pyspark.pandas as ps` instead. \n",
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. Koalas will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/13 02:51:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/opt/bitnami/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets ...Done\n"
     ]
    }
   ],
   "source": [
    "# Boiler plates, do NOT modify\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "from utilities import SEED\n",
    "from utilities import PA2Test\n",
    "from utilities import PA2Data\n",
    "from utilities import data_cat\n",
    "from pa2_main import PA2Executor\n",
    "import time\n",
    "if INPUT_FORMAT == 'dataframe':\n",
    "    import pyspark.ml as M\n",
    "    import pyspark.sql.functions as F\n",
    "    import pyspark.sql.types as T\n",
    "if INPUT_FORMAT == 'koalas':\n",
    "    import databricks.koalas as ks\n",
    "elif INPUT_FORMAT == 'rdd':\n",
    "    import pyspark.mllib as M\n",
    "    from pyspark.mllib.feature import Word2Vec\n",
    "    from pyspark.mllib.linalg import Vectors\n",
    "    from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--py-files utilities.py,assignment2.py \\\n",
    "--deploy-mode client \\\n",
    "pyspark-shell'\n",
    "\n",
    "class args:\n",
    "    review_filename = data_cat.review_filename\n",
    "    product_filename = data_cat.product_filename\n",
    "    product_processed_filename = data_cat.product_processed_filename\n",
    "    ml_features_train_filename = data_cat.ml_features_train_filename\n",
    "    ml_features_test_filename = data_cat.ml_features_test_filename\n",
    "    output_root = '/home/{}/{}-pa2/test_results'.format(getpass.getuser(), PID)\n",
    "    test_results_root = data_cat.test_results_root\n",
    "    pid = PID\n",
    "\n",
    "pa2 = PA2Executor(args, input_format=INPUT_FORMAT)\n",
    "data_io = pa2.data_io\n",
    "data_dict = pa2.data_dict\n",
    "begin = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:52.353249Z",
     "start_time": "2020-01-26T20:45:52.343036Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import your own dependencies\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Bring the part_1 datasets to memory and de-cache part_2 datasets. \n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task0: warm up \n",
    "This task is provided for you to get familiar with Spark API. We will use the dataframe API to demonstrate. Solution is given to you and this task won't be graded.\n",
    "\n",
    "Refer to https://spark.apache.org/docs/latest/api/python/pyspark.sql.html for API guide.\n",
    "\n",
    "The task is to implement the function below. Given the ```product_data``` table:\n",
    "1. Take and print five rows.\n",
    "\n",
    "1. Select only the ```asin``` column, then print five rows of it.\n",
    "\n",
    "1. Select the row where ```asin = B00I8KEOTM``` and print it.\n",
    "\n",
    "1. Count the total number of rows.\n",
    "\n",
    "1. Calculate the mean ```price```.\n",
    "\n",
    "1. You need to conduct the above operations, then extract some statistics out of the generated columns. You need to put the statistics in a python dictionary named ```res```. The description and schema of it are as follows:\n",
    "    ```\n",
    "    res\n",
    "     | -- count_total: int -- count of total rows of the entire table after your operations\n",
    "     | -- mean_price: float -- mean value of column price\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|      asin|           salesRank|          categories|               title|price|             related|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|B00I8HVV6E|{Home &amp; Kitch...|[[Home & Kitchen,...|Intelligent Desig...|27.99|{also_viewed -> [...|\n",
      "|B00I8KEOTM|                null|[[Apps for Androi...|                null| null|{also_viewed -> [...|\n",
      "|B00I8KCW4G|{Clothing -> 2233...|[[Clothing, Shoes...|eShakti Women's P...|41.95|{also_viewed -> [...|\n",
      "|B00I8JKCQW|{Clothing -> 1405...|[[Clothing, Shoes...|Lady Slimming Mid...| null|{also_viewed -> [...|\n",
      "|B00I8JKI8E|{Home &amp; Kitch...|[[Clothing, Shoes...|3 Tier Bangle Bra...|24.99|{also_viewed -> [...|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict[\"product\"].show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:52.368918Z",
     "start_time": "2020-01-26T20:45:52.355018Z"
    }
   },
   "outputs": [],
   "source": [
    "def task_0(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    product_data.show(5)\n",
    "    product_data[['asin']].show(5)\n",
    "    product_data.where(F.col('asin') == 'B00I8KEOTM').show()\n",
    "    count_rows = product_data.count()\n",
    "    mean_price = product_data.select(F.avg(F.col('price'))).head()[0]\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    # Calculate the values programmatically. Do not change the keys and do not\n",
    "    # hard-code values in the dict. Your submission will be evaluated with\n",
    "    # different inputs.\n",
    "    # Modify the values of the following dictionary accordingly.\n",
    "    res = {'count_total': None, 'mean_price': None}\n",
    "    \n",
    "    # Modify res:\n",
    "\n",
    "    res['count_total'] = count_rows\n",
    "    res['mean_price'] = mean_price\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-26T20:45:54.222111Z",
     "start_time": "2020-01-26T20:45:52.370377Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|      asin|           salesRank|          categories|               title|price|             related|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "|B00I8HVV6E|{Home &amp; Kitch...|[[Home & Kitchen,...|Intelligent Desig...|27.99|{also_viewed -> [...|\n",
      "|B00I8KEOTM|                null|[[Apps for Androi...|                null| null|{also_viewed -> [...|\n",
      "|B00I8KCW4G|{Clothing -> 2233...|[[Clothing, Shoes...|eShakti Women's P...|41.95|{also_viewed -> [...|\n",
      "|B00I8JKCQW|{Clothing -> 1405...|[[Clothing, Shoes...|Lady Slimming Mid...| null|{also_viewed -> [...|\n",
      "|B00I8JKI8E|{Home &amp; Kitch...|[[Clothing, Shoes...|3 Tier Bangle Bra...|24.99|{also_viewed -> [...|\n",
      "+----------+--------------------+--------------------+--------------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+\n",
      "|      asin|\n",
      "+----------+\n",
      "|B00I8HVV6E|\n",
      "|B00I8KEOTM|\n",
      "|B00I8KCW4G|\n",
      "|B00I8JKCQW|\n",
      "|B00I8JKI8E|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "|      asin|salesRank|          categories|title|price|             related|\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "|B00I8KEOTM|     null|[[Apps for Androi...| null| null|{also_viewed -> [...|\n",
      "+----------+---------+--------------------+-----+-----+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:=================================================>      (59 + 2) / 67]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_0 --------------------------------------------------------------\n",
      "2/2 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if INPUT_FORMAT == 'dataframe':\n",
    "    res = task_0(data_io, data_dict['product'])\n",
    "    pa2.tests.test(res, 'task_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if INPUT_FORMAT == 'dataframe':\n",
    "#     res = task_0(data_io, data_dict['product'])\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- overall: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- salesRank: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- related: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict[\"review\"].printSchema()\n",
    "data_dict[\"product\"].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:30:44.034299Z",
     "start_time": "2019-12-10T21:30:44.012787Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_1 assignment2.py\n",
    "def task_1(data_io, review_data, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    overall_column = 'overall'\n",
    "    # Outputs:\n",
    "    mean_rating_column = 'meanRating'\n",
    "    count_rating_column = 'countRating'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    groups = review_data.groupby(\"asin\")\n",
    "    \n",
    "    newRev = groups.agg(\n",
    "        F.avg(\"overall\").alias(\"meanRating\"),\n",
    "        F.count(\"overall\").alias(\"countRating\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    newPrd = product_data.join(newRev, \"asin\", \"left\").select(\"asin\", \"meanRating\", \"countRating\")\n",
    "    \n",
    "    vals = newPrd.select(\n",
    "                        F.count(\"asin\").alias(\"count_total\"),\n",
    "                        F.mean(\"meanRating\").alias(\"mean_meanRating\"), \n",
    "                        F.mean(\"countRating\").alias(\"mean_countRating\"),\n",
    "                        F.variance(\"meanRating\").alias(\"variance_meanRating\"), \n",
    "                        F.variance(\"countRating\").alias(\"variance_countRating\"),\n",
    "                        F.count(F.when(F.col(\"meanRating\").isNull() , \"meanRating\")).alias(\"numNulls_meanRating\"),\n",
    "                        F.count(F.when(F.col(\"countRating\").isNull() , \"countRating\")).alias(\"numNulls_countRating\"),\n",
    "                       ).collect()[0]\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    # Calculate the values programmaticly. Do not change the keys and do not\n",
    "    # hard-code values in the dict. Your submission will be evaluated with\n",
    "    # different inputs.\n",
    "    # Modify the values of the following dictionary accordingly.\n",
    "    res = {\n",
    "        'count_total': vals[\"count_total\"],\n",
    "        'mean_meanRating': vals[\"mean_meanRating\"],\n",
    "        'variance_meanRating': vals[\"variance_meanRating\"],\n",
    "        'numNulls_meanRating': vals[\"numNulls_meanRating\"],\n",
    "        'mean_countRating': vals[\"mean_countRating\"],\n",
    "        'variance_countRating': vals[\"variance_countRating\"],\n",
    "        'numNulls_countRating': vals[\"numNulls_countRating\"]\n",
    "    }\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_1')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:04.214179Z",
     "start_time": "2019-12-09T22:18:39.293699Z"
    },
    "deletable": false,
    "editable": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_1 --------------------------------------------------------------\n",
      "Test 1/7 : count_total ... Pass\n",
      "Test 2/7 : mean_countRating ... Pass\n",
      "Test 3/7 : mean_meanRating ... Pass\n",
      "Test 4/7 : numNulls_countRating ... Pass\n",
      "Test 5/7 : numNulls_meanRating ... Pass\n",
      "Test 6/7 : variance_countRating ... Pass\n",
      "Test 7/7 : variance_meanRating ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_1(data_io, data_dict['review'], data_dict['product'])\n",
    "pa2.tests.test(res, 'task_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "\n",
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:16.942833Z",
     "start_time": "2019-12-10T21:31:16.925378Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_2 assignment2.py\n",
    "def task_2(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    salesRank_column = 'salesRank'\n",
    "    categories_column = 'categories'\n",
    "    asin_column = 'asin'\n",
    "    # Outputs:\n",
    "    category_column = 'category'\n",
    "    bestSalesCategory_column = 'bestSalesCategory'\n",
    "    bestSalesRank_column = 'bestSalesRank'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    flat_data = product_data.withColumn(\n",
    "            \"category\", \n",
    "            F.when(\n",
    "                F.col(\"categories\").getItem(0).getItem(0) != '', \n",
    "                F.col(\"categories\").getItem(0).getItem(0)\n",
    "            ).otherwise(None)\n",
    "        ).withColumn(\n",
    "            \"bestSalesCategory\", \n",
    "            F.map_keys(product_data[\"salesRank\"]).getItem(0)\n",
    "        ).withColumn(\n",
    "            \"bestSalesRank\", \n",
    "            F.map_values(product_data[\"salesRank\"]).getItem(0)\n",
    "        ).select('asin', 'category', 'salesRank', 'bestSalesCategory', 'bestSalesRank')\n",
    "    \n",
    "\n",
    "    vals = flat_data.select(\n",
    "        F.count(\"asin\").alias(\"count_total\"),\n",
    "        F.mean(\"bestSalesRank\").alias(\"mean_bestSalesRank\"), \n",
    "        F.variance(\"bestSalesRank\").alias(\"variance_bestSalesRank\"),\n",
    "        F.count(F.when(F.col(\"category\").isNull() , \"category\")).alias(\"numNulls_category\"),\n",
    "        F.count(F.when(F.col(\"bestSalesRank\").isNull() , \"bestSalesRank\")).alias(\"numNulls_bestSalesCategory\"),\n",
    "        F.countDistinct(\"category\").alias(\"countDistinct_category\"),\n",
    "        F.countDistinct(\"bestSalesCategory\").alias(\"countDistinct_bestSalesCategory\"),\n",
    "    ).collect()[0]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': vals['count_total'],\n",
    "        'mean_bestSalesRank': vals['mean_bestSalesRank'],\n",
    "        'variance_bestSalesRank': vals['variance_bestSalesRank'],\n",
    "        'numNulls_category': vals['numNulls_category'],\n",
    "        'countDistinct_category': vals['countDistinct_category'],\n",
    "        'numNulls_bestSalesCategory': vals['numNulls_bestSalesCategory'],\n",
    "        'countDistinct_bestSalesCategory': vals['countDistinct_bestSalesCategory']\n",
    "    }\n",
    "    # Modify res:\n",
    "    print(res)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_2')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:19:19.308187Z",
     "start_time": "2019-12-09T22:19:04.274345Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'mean_bestSalesRank': 1102421.3114573301, 'variance_bestSalesRank': 3616424023407.0415, 'numNulls_category': 270169, 'countDistinct_category': 82, 'numNulls_bestSalesCategory': 2480772, 'countDistinct_bestSalesCategory': 33}\n",
      "tests for task_2 --------------------------------------------------------------\n",
      "Test 1/7 : countDistinct_bestSalesCategory ... Pass\n",
      "Test 2/7 : countDistinct_category ... Pass\n",
      "Test 3/7 : count_total ... Pass\n",
      "Test 4/7 : mean_bestSalesRank ... Pass\n",
      "Test 5/7 : numNulls_bestSalesCategory ... Pass\n",
      "Test 6/7 : numNulls_category ... Pass\n",
      "Test 7/7 : variance_bestSalesRank ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_2(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 3\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:26.542481Z",
     "start_time": "2019-12-10T21:31:26.525050Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_3 assignment2.py\n",
    "def task_3(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    asin_column = 'asin'\n",
    "    price_column = 'price'\n",
    "    attribute = 'also_viewed'\n",
    "    related_column = 'related'\n",
    "    # Outputs:\n",
    "    meanPriceAlsoViewed_column = 'meanPriceAlsoViewed'\n",
    "    countAlsoViewed_column = 'countAlsoViewed'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    also_viewed = product_data.select(\n",
    "        product_data['asin'],\n",
    "        product_data[\"related\"].getItem('also_viewed').alias('also_viewed'),\n",
    "        F.when(\n",
    "            F.size(product_data[\"related\"].getItem('also_viewed')) > 0,\n",
    "            F.size(product_data[\"related\"].getItem('also_viewed'))\n",
    "        ).otherwise(None).alias('countAlsoViewed')\n",
    "    )\n",
    "\n",
    "    exploded_vals = also_viewed.withColumn(\n",
    "        \"exp_vals\",\n",
    "        F.explode(F.col('also_viewed'))\n",
    "    ).select('asin', 'exp_vals')\n",
    "#     exploded_vals.show(10)\n",
    "    \n",
    "    prices = product_data.select(F.col('asin').alias('pasin'), 'price').filter(F.col('price').isNotNull())\n",
    "#     prices.show(10)\n",
    "\n",
    "    exp_prices = prices.join(exploded_vals,prices.pasin == exploded_vals.exp_vals,'left')\n",
    "#     exp_prices.show(10)\n",
    "    \n",
    "    merged_prices = exp_prices.groupby('asin').agg(\n",
    "        F.mean('price').alias('meanPriceAlsoViewed')\n",
    "    )\n",
    "#     merged_prices.show(10)\n",
    "    outcome = also_viewed.join(merged_prices, on = 'asin', how = 'left')\n",
    "#     outcome.show(10)\n",
    "    \n",
    "    vals = outcome.select(\n",
    "        F.count(\"asin\").alias(\"count_total\"),\n",
    "        F.mean(\"meanPriceAlsoViewed\").alias(\"mean_meanPriceAlsoViewed\"),\n",
    "        F.mean(\"countAlsoViewed\").alias(\"mean_countAlsoViewed\"),\n",
    "        F.variance(\"meanPriceAlsoViewed\").alias(\"variance_meanPriceAlsoViewed\"),\n",
    "        F.variance(\"countAlsoViewed\").alias(\"variance_countAlsoViewed\"),\n",
    "        F.count(F.when(F.col(\"meanPriceAlsoViewed\").isNull() , \"meanPriceAlsoViewed\")).alias(\"numNulls_meanPriceAlsoViewed\"),\n",
    "        F.count(F.when(F.col(\"countAlsoViewed\").isNull() , \"countAlsoViewed\")).alias(\"numNulls_countAlsoViewed\"),\n",
    "    ).collect()[0]\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': vals['count_total'],\n",
    "        'mean_meanPriceAlsoViewed': vals['mean_meanPriceAlsoViewed'],\n",
    "        'variance_meanPriceAlsoViewed': vals['variance_meanPriceAlsoViewed'],\n",
    "        'numNulls_meanPriceAlsoViewed': vals['numNulls_meanPriceAlsoViewed'],\n",
    "        'mean_countAlsoViewed': vals['mean_countAlsoViewed'],\n",
    "        'variance_countAlsoViewed': vals['variance_countAlsoViewed'],\n",
    "        'numNulls_countAlsoViewed': vals['numNulls_countAlsoViewed']\n",
    "    }\n",
    "    # Modify res:\n",
    "    print(res)\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_3')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:41.442745Z",
     "start_time": "2019-12-09T22:19:19.358780Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'mean_meanPriceAlsoViewed': 45.28501828302965, 'variance_meanPriceAlsoViewed': 5135.311790051096, 'numNulls_meanPriceAlsoViewed': 5835773, 'mean_countAlsoViewed': 31.561228904913023, 'variance_countAlsoViewed': 562.8028467039506, 'numNulls_countAlsoViewed': 5408566}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_3 --------------------------------------------------------------\n",
      "Test 1/7 : count_total ... Pass\n",
      "Test 2/7 : mean_countAlsoViewed ... Pass\n",
      "Test 3/7 : mean_meanPriceAlsoViewed ... Pass\n",
      "Test 4/7 : numNulls_countAlsoViewed ... Pass\n",
      "Test 5/7 : numNulls_meanPriceAlsoViewed ... Pass\n",
      "Test 6/7 : variance_countAlsoViewed ... Pass\n",
      "Test 7/7 : variance_meanPriceAlsoViewed ... Pass\n",
      "7/7 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_3(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- salesRank: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- categories: array (nullable = true)\n",
      " |    |-- element: array (containsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      " |-- related: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: array (valueContainsNull = true)\n",
      " |    |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict['product'].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:31:39.503390Z",
     "start_time": "2019-12-10T21:31:39.484724Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_4 assignment2.py\n",
    "def task_4(data_io, product_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    price_column = 'price'\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    meanImputedPrice_column = 'meanImputedPrice'\n",
    "    medianImputedPrice_column = 'medianImputedPrice'\n",
    "    unknownImputedTitle_column = 'unknownImputedTitle'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    price = product_data.withColumn(\n",
    "        'price', \n",
    "        F.col('price').cast('float')\n",
    "    ).select('asin', 'price', 'title')\n",
    "\n",
    "    price_agg = price.agg(\n",
    "        F.mean('price').alias('mean'), \n",
    "        F.expr('percentile_approx(price, 0.5)').alias('median')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    price_data = price.select(\n",
    "        'asin',\n",
    "        F.when(\n",
    "            F.col('price').isNull(), \n",
    "            price_agg['mean']\n",
    "        ).otherwise(F.col('price')).alias('meanInputedPrice'),\n",
    "        F.when(\n",
    "            F.col('price').isNull(), \n",
    "            price_agg['median']\n",
    "        ).otherwise(F.col('price')).alias('medianInputedPrice'),\n",
    "        F.when(\n",
    "            (F.col('title') == '' ) | F.col('title').isNull(), \n",
    "            'unknown'\n",
    "        ).otherwise(F.col('title')).alias('unknownInputedTitle')\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    vals = price_data.select(\n",
    "        F.count(\"asin\").alias(\"count_total\"),\n",
    "        F.mean(\"meanInputedPrice\").alias(\"mean_meanImputedPrice\"),\n",
    "        F.mean(\"medianInputedPrice\").alias(\"mean_medianImputedPrice\"),\n",
    "        F.variance(\"meanInputedPrice\").alias(\"variance_meanImputedPrice\"),\n",
    "        F.variance(\"medianInputedPrice\").alias(\"variance_medianImputedPrice\"),\n",
    "        F.count(F.when(F.col(\"meanInputedPrice\").isNull() , \"meanInputedPrice\")).alias(\"numNulls_meanImputedPrice\"),\n",
    "        F.count(F.when(F.col(\"medianInputedPrice\").isNull() , \"medianInputedPrice\")).alias(\"numNulls_medianImputedPrice\"),\n",
    "        F.count(F.when(F.col(\"unknownInputedTitle\") == 'unknown' , \"unknownInputedTitle\")).alias(\"numUnknowns_unknownImputedTitle\")\n",
    "    ).collect()[0]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': vals['count_total'],\n",
    "        'mean_meanImputedPrice': vals['mean_meanImputedPrice'],\n",
    "        'variance_meanImputedPrice': vals['variance_meanImputedPrice'],\n",
    "        'numNulls_meanImputedPrice': vals['numNulls_meanImputedPrice'],\n",
    "        'mean_medianImputedPrice': vals['mean_medianImputedPrice'],\n",
    "        'variance_medianImputedPrice': vals['variance_medianImputedPrice'],\n",
    "        'numNulls_medianImputedPrice': vals['numNulls_medianImputedPrice'],\n",
    "        'numUnknowns_unknownImputedTitle': vals['numUnknowns_unknownImputedTitle']\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "    print(res)\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_4')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n",
    "# task_4(data_io, data_dict['product'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:20:47.953226Z",
     "start_time": "2019-12-09T22:20:41.523379Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'mean_meanImputedPrice': 34.93735609455432, 'variance_meanImputedPrice': 3265.3113437199577, 'numNulls_meanImputedPrice': 0, 'mean_medianImputedPrice': 27.815470873162308, 'variance_medianImputedPrice': 3356.652886505402, 'numNulls_medianImputedPrice': 0, 'numUnknowns_unknownImputedTitle': 1432648}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 205:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests for task_4 --------------------------------------------------------------\n",
      "Test 1/8 : count_total ... Pass\n",
      "Test 2/8 : mean_meanImputedPrice ... Pass\n",
      "Test 3/8 : mean_medianImputedPrice ... Pass\n",
      "Test 4/8 : numNulls_meanImputedPrice ... Pass\n",
      "Test 5/8 : numNulls_medianImputedPrice ... Pass\n",
      "Test 6/8 : numUnknowns_unknownImputedTitle ... Pass\n",
      "Test 7/8 : variance_meanImputedPrice ... Pass\n",
      "Test 8/8 : variance_medianImputedPrice ... Pass\n",
      "8/8 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_4(data_io, data_dict['product'])\n",
    "pa2.tests.test(res, 'task_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:29.284661Z",
     "start_time": "2019-12-10T21:32:29.267237Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_5 assignment2.py\n",
    "def task_5(data_io, product_processed_data, word_0, word_1, word_2):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    title_column = 'title'\n",
    "    # Outputs:\n",
    "    titleArray_column = 'titleArray'\n",
    "    titleVector_column = 'titleVector'\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "\n",
    "    splf = F.udf(lambda row: row.lower())\n",
    "\n",
    "    product_processed_data_output = product_processed_data.withColumn(titleArray_column, \n",
    "                                        F.split(splf(title_column), \" \"))\n",
    "\n",
    "    \n",
    "    word2Vec = M.feature.Word2Vec(minCount=100, vectorSize=16, seed=SEED, numPartitions=4, inputCol=titleArray_column, outputCol=titleVector_column)\n",
    "    \n",
    "    model = word2Vec.fit(product_processed_data_output.select(F.col(titleArray_column)))\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': None,\n",
    "        'size_vocabulary': None,\n",
    "        'word_0_synonyms': [(None, None), ],\n",
    "        'word_1_synonyms': [(None, None), ],\n",
    "        'word_2_synonyms': [(None, None), ]\n",
    "    }\n",
    "    # Modify res:\n",
    "    res['count_total'] = product_processed_data_output.count()\n",
    "    res['size_vocabulary'] = model.getVectors().count()\n",
    "    for name, word in zip(\n",
    "        ['word_0_synonyms', 'word_1_synonyms', 'word_2_synonyms'],\n",
    "        [word_0, word_1, word_2]\n",
    "    ):\n",
    "        res[name] = model.findSynonymsArray(word, 10)\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_5')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:26:05.015529Z",
     "start_time": "2019-12-09T22:20:47.999834Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "res = task_5(data_io, data_dict['product_processed'], 'piano', 'rice', 'laptop')\n",
    "pa2.tests.test(res, 'task_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict['product_processed'].printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------+\n",
      "|      asin|               title|category|\n",
      "+----------+--------------------+--------+\n",
      "|0002553112|The last house of...|   Books|\n",
      "+----------+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict['product_processed'].show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-10T21:32:39.991460Z",
     "start_time": "2019-12-10T21:32:39.974136Z"
    }
   },
   "outputs": [],
   "source": [
    "# %load -s task_6 assignment2.py\n",
    "def task_6(data_io, product_processed_data):\n",
    "    # -----------------------------Column names--------------------------------\n",
    "    # Inputs:\n",
    "    category_column = 'category'\n",
    "    # Outputs:\n",
    "    categoryIndex_column = 'categoryIndex'\n",
    "    categoryOneHot_column = 'categoryOneHot'\n",
    "    categoryPCA_column = 'categoryPCA'\n",
    "    # -------------------------------------------------------------------------    \n",
    "\n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    # Apply StringIndexer tp category column\n",
    "    cat_indexer = M.feature.StringIndexer(\n",
    "        inputCol = 'category',\n",
    "        outputCol = 'category_index'\n",
    "    )\n",
    "    prod_processed = cat_indexer.fit(product_processed_data).transform(product_processed_data)\n",
    "    \n",
    "    # Apply OneHotEncoder to indexed column \n",
    "    ohe = M.feature.OneHotEncoder(\n",
    "        inputCol = 'category_index',\n",
    "        outputCol = 'categoryOneHot',\n",
    "        dropLast = False\n",
    "    )\n",
    "    prod_onehot = ohe.fit(prod_processed).transform(prod_processed)\n",
    "\n",
    "    # Apply PCA to ohe column\n",
    "    pca = M.feature.PCA(\n",
    "        k = 15,\n",
    "        inputCol = 'categoryOneHot',\n",
    "        outputCol = 'categoryPCA'\n",
    "    )\n",
    "    output = pca.fit(prod_onehot).transform(prod_onehot)\n",
    "#     output.show()\n",
    "    \n",
    "    # get vals\n",
    "    summarizer = M.stat.Summarizer.metrics(\"mean\")\n",
    "    vals = output.select(\n",
    "        F.count(\"asin\").alias(\"count_total\"),\n",
    "        summarizer.summary(F.col('categoryOneHot')).alias('meanVector_categoryOneHot'),\n",
    "        summarizer.summary(F.col('categoryPCA')).alias('meanVector_categoryPCA')\n",
    "    ).collect()[0]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'count_total': vals['count_total'],\n",
    "        'meanVector_categoryOneHot': vals['meanVector_categoryOneHot'][0],\n",
    "        'meanVector_categoryPCA': vals['meanVector_categoryPCA'][0]\n",
    "    }\n",
    "    # Modify res:\n",
    "    print(res)\n",
    "\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_6')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------\n",
    "# task_6(data_io, data_dict['product_processed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-09T22:29:57.717617Z",
     "start_time": "2019-12-09T22:29:51.132434Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count_total': 9430000, 'meanVector_categoryOneHot': DenseVector([0.2513, 0.1523, 0.0562, 0.0525, 0.0521, 0.0463, 0.0368, 0.0356, 0.0351, 0.0287, 0.0286, 0.0285, 0.0278, 0.0275, 0.0207, 0.0182, 0.0142, 0.0124, 0.0117, 0.0115, 0.0076, 0.0073, 0.0065, 0.0052, 0.005, 0.0032, 0.0026, 0.0019, 0.0012, 0.0011, 0.0008, 0.0008, 0.0007, 0.0007, 0.0006, 0.0005, 0.0005, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]), 'meanVector_categoryPCA': DenseVector([-0.15, -0.1752, 0.0165, -0.0027, 0.0334, -0.0537, 0.0099, -0.0046, 0.04, -0.0003, -0.0019, 0.0061, -0.0037, 0.0372, -0.0305])}\n",
      "tests for task_6 --------------------------------------------------------------\n",
      "Test 1/9 : count_total ... Pass\n",
      "Test 2/9 : meanVector_categoryOneHot-length ... Pass\n",
      "Test 3/9 : meanVector_categoryOneHot-sum ... Pass\n",
      "Test 4/9 : meanVector_categoryOneHot-mean ... Pass\n",
      "Test 5/9 : meanVector_categoryOneHot-variance ... Pass\n",
      "Test 6/9 : meanVector_categoryPCA-length ... Pass\n",
      "Test 7/9 : meanVector_categoryPCA-sum ... Pass\n",
      "Test 8/9 : meanVector_categoryPCA-mean ... Pass\n",
      "Test 9/9 : meanVector_categoryPCA-variance ... Pass\n",
      "9/9 passed\n",
      "-------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = task_6(data_io, data_dict['product_processed'])\n",
    "pa2.tests.test(res, 'task_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T21:23:18.882119Z",
     "start_time": "2019-11-26T21:23:18.873162Z"
    },
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring the part_2 datasets to memory and de-cache part_1 datasets.\n",
    "# Execute this once before you start working on this Part\n",
    "data_dict, _ = data_io.cache_switch(data_dict, 'part_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_7(data_io, train_data, test_data):\n",
    "    \n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    dt = M.regression.DecisionTreeRegressor(maxDepth=5, featuresCol=\"features\", labelCol=\"overall\")\n",
    "    \n",
    "    model = dt.fit(train_data)\n",
    "    \n",
    "    result = model.transform(test_data)\n",
    "    \n",
    "    result = result.withColumn('SE', F.pow(F.col(\"overall\") - F.col(\"prediction\"), 2))\n",
    "    \n",
    "    MSE = result.select(F.mean(F.col(\"SE\"))).collect()[0][0]\n",
    "    \n",
    "    RMSE = math.sqrt(MSE)\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'test_rmse': RMSE\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_7')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = task_7(data_io, data_dict['ml_features_train'], data_dict['ml_features_test'])\n",
    "pa2.tests.test(res, 'task_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_8(data_io, train_data, test_data):\n",
    "    \n",
    "    # ---------------------- Your implementation begins------------------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    # ---------------------- Put results in res dict --------------------------\n",
    "    res = {\n",
    "        'test_rmse': None,\n",
    "        'valid_rmse_depth_5': None,\n",
    "        'valid_rmse_depth_7': None,\n",
    "        'valid_rmse_depth_9': None,\n",
    "        'valid_rmse_depth_12': None,\n",
    "    }\n",
    "    # Modify res:\n",
    "\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------- Do not change -----------------------------\n",
    "    data_io.save(res, 'task_8')\n",
    "    return res\n",
    "    # -------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = task_8(data_io, data_dict['ml_features_train'], data_dict['ml_features_test'])\n",
    "pa2.tests.test(res, 'task_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"End to end time: {}\".format(time.time()-begin))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
